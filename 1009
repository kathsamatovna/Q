Perfect — that’s actually a really realistic constraint, and your reasoning is spot-on.
If traders don’t directly control or even know the number of child orders, then the “slicing” part is a consequence of the algorithm, not an explicit decision variable.
So your analysis should shift from “did they plan the child structure?” to “did they time when and how aggressively they placed the parent so that the resulting pattern looks impact-minimising?”

Here’s how to reason through it, step-by-step, given exactly what you have:


---

1️⃣ What you actually observe

For each parent (or each trader × side × symbol × parentID):

timestamps of each child fill (or at least start + end times),

the ordered price (limit or market request),

the executed price,

mid price of the order book at those times,

total quantity ordered vs executed,

algoType, and maybe venue.


That’s enough for quite a rich picture, even without full depth or spreads.


---

2️⃣ What “impact minimisation” would look like in this data

You’re looking for behavioural fingerprints of traders/algo that react to liquidity rather than forcing the market.
Since they can’t pick the number of child fills, what they can influence is when and how large their parent order is sent.

You’d expect to see patterns like:

📉 Low pre-trade price momentum — they initiate trades when the mid price is flat, not trending.

⚖️ Symmetry around mid — execution prices cluster tightly around the mid; small signed slippage.

🕐 Execution bursts coincide with quiet periods — fewer overlapping orders, low volatility.

🔁 Adaptive pacing — intervals between child fills shrink when liquidity/volume increases (reactive algos).

🧩 AlgoType–environment alignment — passive algos fire when market is calm; aggressive when the mid is moving.


So the question becomes: do their trades systematically align with low-volatility, low-impact windows, and do certain algos achieve smaller signed slippage in those windows?


---

3️⃣ What to cluster

Option A — “Execution episode” (metaorder) clusters

Use time proximity + trader + side + symbol to group bursts:

features for clustering:
[start_time, duration, total_qty, avg_signed_slippage, algoType_encoded]

optional: add simple temporal features like hour of day or volatility in the 5 min before start (from mid price).


Result: each cluster = one “execution episode”.

Then you can compare cluster-level stats:

mean slippage

average pre-trade volatility

algoType mix


Patterns: clusters that happen in low-vol windows with small mean slippage → timing behaviour.

Option B — “Market-state” clusters

Cluster all time points (or 5 min buckets) using market conditions you can compute from mid:

rolling volatility, return, maybe recent trade density, hour-of-day.
→ gives you “liquidity regimes” (calm vs volatile periods).


Then look at when traders initiate orders across those regimes. If executions concentrate in calm regimes, that’s deliberate timing.


---

4️⃣ What to plot

🔹 A. Timing heatmap

x-axis = time-of-day,
y-axis = rolling-volatility bin (from mid price),
color = count of new parent orders (or share of volume).
If colour concentrates in low-vol cells → deliberate timing.


---

🔹 B. Slippage vs volatility

Scatter or boxplot:
x = pre-trade volatility (5 min window from mid),
y = signed slippage (% or pips),
color = algoType.
If slopes are flat or negative (higher vol → same or smaller slippage), algos are adapting well.
If slope is strongly positive → they’re walking into noise or impact.


---

🔹 C. Cumulative mid-price impact

For each trade: plot Δmid(t + 1 min) vs signed notional or size.
A weak or near-zero relationship → good impact control.


---

🔹 D. Duration vs size (post-hoc slicing pattern)

Even if they don’t control number of children, algos that are gentler will stretch duration as size grows. Plot log(size) vs duration, colour by algoType; flatter slopes for some algos = better liquidity matching.


---

🔹 E. AlgoType performance within environment

Bin episodes by pre-trade volatility decile.
For each bin, plot mean or median slippage per algoType.
Consistently lowest slippage lines per environment → that algo (and its traders) are impact-aware.


---

5️⃣ Summary of logical flow

Step	Object	Goal

1	Compute mid-based volatility & trend measures	Quantify the “state of market” at each potential trade time
2	Detect parent/episode clusters	Group related fills
3	Compute features per episode	start time, duration, size, slippage, algoType, pre-trade vol
4	Cluster or bin episodes by environment	Calm vs volatile regimes
5	Compare behaviour	Which algos/traders trade when, and what slippage results
6	Plot A–E	Visual patterns of timing & adaptation



---

6️⃣ How you’d reason about conclusions

If metaorders systematically start in low-vol windows, and their slippage is smaller than random times → timing is deliberate.

If larger trades stretch over longer durations but keep slippage constant → impact-aware pacing.

If certain algos consistently outperform peers in same volatility bin → effective algo selection.

If none of that holds → execution may be random or urgency-driven.



---

7️⃣ Bonus reasoning trick

Think of the mid-price as a continuous “liquidity thermometer”.
Whenever a trader sends an order, freeze a snapshot:

what was volatility over last N seconds?

how much did mid move after the trade?
If you later see that trades mostly occur when volatility is cool and price impact flat, you’ve caught timing behaviour — even without any full book data.



---

If you want, I can sketch the feature definitions and minimal plotting recipe (rolling vol, slippage metric, duration) in code, built purely from your mid-price and execution table — would you like that next?

