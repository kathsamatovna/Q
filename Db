Ah—got it: you mean DBSCAN (the clustering algorithm), not a database scan. My bad earlier.

Here’s why your boss’s “cumulative count / step chart” ask still fits DBSCAN:

DBSCAN needs a sense of density. A cumulative-orders step chart (time on x, running count on y) lets you eyeball arrival-rate bursts vs plateaus.

Steep stairs ⇒ high local density (good candidate clusters).

Long flats ⇒ sparse regions (likely noise).
That quick visual helps decide whether DBSCAN is appropriate and roughly how to choose ε (scale of “neighbourhood”) and minPts (what counts as dense).


Heuristics you can get from the chart

Typical inter-arrival time in “busy” segments ≈ a ballpark for ε (in time units, or after scaling with other features).

Natural burst sizes (steps per short interval) hint at a sane minPts.



Quick q snippets to prep for DBSCAN

Assume a table orders with at least: time (timestamp), trader, parent (parent order id), orderId, qty, orderSize.

1) Step chart data (running count)

/ Sort by time first
t: `time xasc orders

/ Cumulative counts (overall)
stepAll: select time, cum:sums 1 from t

/ Cumulative per trader (or per trader,parent)
stepByTrader: select time, trader, cum:sums 1 by trader from t
stepByTP:     select time, trader, parent, cum:sums 1 by trader,parent from t

Plot any of those to see the “stairs”.

2) Features that reflect density/bursts

/ Inter-arrival times per trader (proxy for local density)
feat: update dt: time - prev time by trader from t;

/ New-parent flags and split intensity
feat: update newParent: parent<>prev parent by trader, frac: qty%orderSize from feat;

/ (Optional) rate proxy over short windows: count since last 5 minutes
/ Simple proxy: how many orders since last k minutes per trader
/ Mark orders within a rolling window using time differences:
k: 00:05:00;
feat: update winCount: sums (dt<k) by trader from feat;  / rough burstiness indicator

> Tip: Standardise features before DBSCAN (z-scores) so time-based and size-based signals are comparable.



Choosing DBSCAN parameters (fast)

Start with minPts ≈ 5–20 depending on how “big” a burst should be to count as a cluster (trader-level vs parent-level).

Compute a k-distance plot (k = minPts−1) on your chosen feature space (e.g., [dt, frac, qty] after scaling). Look for the elbow ⇒ ε.

If bursts happen at multiple scales, consider HDBSCAN (handles variable density).


Running DBSCAN from kdb+

kdb+/q doesn’t have DBSCAN built-in. Two common routes:

EmbedPy + scikit-learn to run DBSCAN(eps=..., min_samples=...) on your feature matrix and bring labels back into q.

Or export features to CSV/Parquet, run DBSCAN in Python/R, and rejoin labels in q on orderId.


If you want, I can sketch the tiny EmbedPy glue to push feat → Python, run sklearn.cluster.DBSCAN, and join the label column back in q.

Great question — choosing eps and minPts is the tuning step in DBSCAN.
Here’s how you can do it in a principled way:


---

1. minPts (minimum points per cluster)

Rule of thumb:

minPts ≈ dimensionality + 1 (so for 2D data, at least 3; for 5D, at least 6).

But in practice:

Small clusters of interest? keep minPts low (5–10).

Want only strong dense bursts? set minPts higher (20–50).



Domain-driven tweak:

For trader order flow, if a “burst” is usually ≥10 child orders, start minPts=10.

If you want to capture even micro-bursts, try minPts=3–5.




---

2. eps (neighbourhood radius)

k-distance plot method:

1. Choose k = minPts − 1.


2. For each point, compute distance to its k-th nearest neighbour.


3. Sort those distances, plot them.


4. Look for the “elbow” (sharp bend) → good candidate for eps.



This works because:

Distances in dense regions plateau (low).

Distances in sparse regions jump (noise).

The elbow is the threshold that separates the two.


Scaling matters:

If you mix features (time gaps, sizes, ratios), scale them (z-score or min-max). Otherwise, one unit (say seconds) may dwarf another (order fraction).

After scaling, eps is in unitless z-score space, so the elbow is easier to spot.




---

3. Putting it together

Start with minPts=5–10.

Generate the k-distance plot → pick elbow as eps.

Run DBSCAN, check clusters.

If:

Too many noise points: increase eps slightly.

Clusters merging too much: decrease eps or raise minPts.




---

4. Extra heuristics for your case (orders)

If you cluster order arrival times (e.g., inter-arrival in seconds):

eps roughly = “max gap that still counts as same burst.”

e.g., if bursts are within 30 sec, set eps ≈ 30s.


If clustering child orders under parent orders:

minPts ≈ min number of child orders that define a “strategic split.”


If clustering trader behaviour across days:

Scale features (volume, time, fraction filled) before applying DBSCAN.




---

👉 Do you want me to show you exactly how to generate the k-distance plot (say in Python via scikit-learn, or in q using EmbedPy), so you can visually pick eps?

