Perfect â€” this is exactly the kind of refinement that makes your ST-DBSCAN analysis much more interpretable and statistically robust. Letâ€™s break it down cleanly into two complementary parts:


---

ðŸ§  PART 1: Evaluating Your ST-DBSCAN Clusters

âž¤ Silhouette Score

Silhouette measures how well each point fits within its cluster vs. others.
Formulaically  , where  = intra-cluster distance,  = nearest-cluster distance.

> âš ï¸ Use this only on dense numeric features (e.g. scaled order qty + price).
If you mix categorical fields or time indices, it becomes meaningless.



from sklearn.metrics import silhouette_score
import numpy as np

def evaluate_clusters(df, qty_col='grossOrdered', price_col='priceExecuted', cluster_col='cluster'):
    valid = df[df[cluster_col] >= 0].copy()              # drop noise
    if valid[cluster_col].nunique() < 2:
        return np.nan, np.nan                            # silhouette requires â‰¥2 clusters

    X = valid[[qty_col, price_col]].apply(pd.to_numeric, errors='coerce').fillna(0.0).to_numpy()
    score = silhouette_score(X, valid[cluster_col])
    return score

Usage:

sil = evaluate_clusters(df_clusters)
print(f"Silhouette score: {sil:.3f}")


---

âž¤ Daviesâ€“Bouldin (Bougainvillea?) Score

I assume you meant Davies-Bouldin, not Bougainvillea ðŸ™‚.
It measures average cluster scatter / separation â€” lower is better.

from sklearn.metrics import davies_bouldin_score

def db_index(df, qty_col='grossOrdered', price_col='priceExecuted', cluster_col='cluster'):
    valid = df[df[cluster_col] >= 0].copy()
    if valid[cluster_col].nunique() < 2:
        return np.nan
    X = valid[[qty_col, price_col]].apply(pd.to_numeric, errors='coerce').fillna(0.0).to_numpy()
    return davies_bouldin_score(X, valid[cluster_col])

Usage:

dbs = db_index(df_clusters)
print(f"Daviesâ€“Bouldin score: {dbs:.3f}")


---

ðŸ“¦ PART 2: Bucketing Orders by Size (or Any Feature)

This lets you ask â€œDo small vs. large orders cluster differently?â€

âž¤ Example 1 â€“ Fixed-Width Buckets

bins = [0, 1e6, 5e6, 10e6, np.inf]
labels = ['small','medium','large','huge']
df['sizeBucket'] = pd.cut(df['grossOrdered'], bins=bins, labels=labels)

âž¤ Example 2 â€“ Quantile Buckets (equal-count)

df['sizeBucket'] = pd.qcut(df['grossOrdered'], q=4, labels=['Q1','Q2','Q3','Q4'])


---

âž¤ Cluster Separately per Bucket

You can extend your earlier loop to add a third grouping key:

out = []
for (bkt, side, bucket), g in df.groupby(['bucket5','dealSide','sizeBucket'], sort=False):
    if len(g) < 5:
        continue
    X = g[['grossOrdered','priceExecuted']].apply(pd.to_numeric, errors='coerce').fillna(0.0).to_numpy()
    scaler = RobustScaler().fit(X)
    g_clustered = ST_DBSCAN(g, eps_space=0.8, eps_time_s=300,
                            min_samples=5, qty_col='grossOrdered',
                            price_col='priceExecuted', side_col='dealSide',
                            scaler=scaler)
    g_clustered['sizeBucket'] = bucket
    out.append(g_clustered)

df_clusters = pd.concat(out)

Now you can:

scores = (df_clusters.groupby('sizeBucket')
          .apply(lambda g: pd.Series({
              'silhouette': evaluate_clusters(g),
              'davies_bouldin': db_index(g),
              'n_clusters': g['cluster'].nunique()
          })))
print(scores)


---

ðŸ“ˆ Optional â€“ Visualize Quality vs. Bucket

import matplotlib.pyplot as plt

plt.figure()
plt.bar(scores.index, scores['silhouette'])
plt.title("Silhouette by Order-Size Bucket")
plt.ylabel("Score")
plt.show()


---

ðŸ” Summary

Goal	Function / Step

Quantify cohesion/separation	silhouette_score, davies_bouldin_score
Bucket orders by notional	pd.cut or pd.qcut
Run ST-DBSCAN within buckets	Group by bucket5 Ã— dealSide Ã— sizeBucket
Compare cluster quality	Group + evaluate



---

Would you like me to modify your existing notebook loop (the one that currently buckets only by bucket5 and dealSide) so that it automatically adds size bucketing + evaluation + plots in one go?
